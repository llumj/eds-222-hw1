---
title: "EDS 222: Homework 1"
date: "10/13/2024"
author: "Joshua Mull"
editor_options: 
  chunk_output_type: console
execute:
  warning: false
format: 
  html: 
    code-fold: true
---


## Background

*(The case study in this exercise is based on reality, but does not include actual observational data.)*

In this exercise we will look at a case study concerning air quality in South Asia. The World Health Organization estimates that air pollution kills an estimated seven million people per year, due to its effects on the cardiovascular and respiratory systems. Out of the 40 most polluted cities in the world, South Asia is home to 37, and Pakistan was ranked to contain the second most air pollution in the world in 2020 (IQAIR, 2020). In 2019, Lahore, Pakistan was the 12th most polluted city in the world, exposing a population of 11.1 million people to increased mortality and morbidity risks.

In this exercise, you are given two datasets from Lahore, Pakistan and are asked to compare the two different data collection strategies from this city. These data are:

-   Crowd-sourced data from air quality monitors located in people's homes. These data are voluntarily collected by individual households who choose to install a monitor in their home and upload their data for public access.

-   Official government data from monitors installed by government officials at selected locations across Lahore. There have been reports that government officials strategically locate monitors in locations with cleaner air in order to mitigate domestic and international pressure to clean up the air.

::: callout-note
All data for EDS 222 will be stored on the Taylor server, in the shared `/courses/eds-222/data/` directory. Please see material from EDS 214 on how to access and retrieve data from Taylor. These data are small; all compute can be handled locally. Thanks to Bren PhD student Fatiq Nadeem for assembling these data!
:::

In answering the following questions, please consider the lecture content from class on sampling strategies, as well as the material in Chapter 2 of [*Introduction to Modern Statistics*](https://openintro-ims.netlify.app/data-design). Include in your submission your version of this file "`eds-222-hw1.qmd`" and the rendered HTML output, each containing complete answers to all questions *as well as the associated code*. Questions with answers unsupported by the code will be marked incomplete. Showing your work this way will help you develop the habit of creating reproducible code.

## Assessment

### Question 1

#### Load Libraries 
```{r echo=TRUE}
library(tidyverse)
library(here)
```

Load the data from each source and label it as `crowdsourced` and `govt` accordingly. For example:

``` {r}
crowdsourced <- readRDS(file.path("data", "airpol-PK-crowdsourced.RDS"))
govt <- readRDS(file.path("data", "airpol-PK-govt.RDS"))
```

::: callout-warning
There's an implicit assumption about file organization in the code above. What is it? How can you make the code work?

I noticed that the RDS files were stored in a folder called data. I needed ot make a folder called data to match the file path and load the files. 
:::

1.  These dataframes have one row per pollution observation. How many pollution records are in each dataset?

There are 5488 rows in crowdsourced and 1960 rows in govt. 

```{r}
# Use nrow to search number of rows in data frames
print(paste("Crowdsource has",nrow(crowdsourced), "rows"))

print(paste("Govt has",nrow(govt), "rows"))
```

2.  Each monitor is located at a unique latitude and longitude location. How many unique monitors are in each dataset?

For this question I did it two different ways. First, I used groupby() and mutated a new column. Then, I printed the length of unique values in that series. 

The second way I used distinct() to find unique pairs in lat and long and then counted them. This was seemed cleaner in code. 

```{r}
# Use groupby() and mutate to find the number of unique combinations and store them in a new series called lat_long 
unique_crowdsourced <- crowdsourced %>%
  group_by(latitude, longitude) %>%
  mutate(lat_long = cur_group_id()) 
  
# Use unique() to find the number of unique values in lat_long. Use length to count the number of unique pairs. Print 
print(paste("The number of unique values in crowdsource lat_long is",length(unique(unique_crowdsourced$lat_long))))

# Use distinct to find the unique combinations of latitude and longitude and Count() to count them
unique_govt <- govt %>%
  distinct(latitude, longitude) %>%
  count()

# Print 
print(paste("The number of unique values in govt lat_long is",unique_govt))

```


::: callout-tip
`group_by(longitude,latitude)` and `cur_group_id()` in `dplyr` will help in creating a unique identifier for each (longitude, latitude) pair.
:::

### Question 2

The goal of pollution monitoring in Lahore is to measure the average pollution conditions across the city.

1.  What is the *population* in this setting? Please be precise.

The population is the entire pollution across Lahore. I.e. every single place would have a pollution associated with its location. 

2.  What are the *samples* in this setting? Please be precise.

The sample is the select locations across lahore. Some in peoples homes voluntarily and some from government agencies. 

3.  These samples were not randomly collected from across locations in Lahore. Given the sampling approaches described above, discuss possible biases that may enter when we use these samples to construct estimates of population parameters.

There could be numerous biases in the data offered. Some included disproportionate pollution numbers based on where sensors are location. Voluntary houses with sensors could be more affluent than others who did not participate. Better insulated homes could skew the numbers to show less pollution. Government sensors have been suspected of biased loactions to show better air quality for Lahore. 

### Question 3

1.  For both the government data and the crowd-sourced data, report the sample mean, sample minimum, and sample maximum value of PM 2.5 (measured in $\mu g/m^3$).

```{r}
# Use mean and buck to find the mean of PM series in crowdsourced data frame
mean_crowsourced_pm <- mean(crowdsourced$PM)

# Print
print(paste("Crowsourced PM mean is", round(mean_crowsourced_pm, 2), "µg/m³"))

# Use mean and buck to find the mean of PM series in govt data frame
mean_govt_pm <- mean(govt$PM)

# Print 
print(paste("Governemnt PM mean is", round(mean_govt_pm, 2), "µg/m³"))

# Find PM sample min of crowdsourced
min_crowsourced_pm <- min(crowdsourced$PM)

# Print
print(paste("Crowdsourced PM min is", min_crowsourced_pm, "µg/m³"))

# Find PM sample min of govt
min_govt_pm <- min(govt$PM)

# Print 
print(paste("Government PM min is", min_govt_pm, "µg/m³"))

# Find Pm sample max of crowdsource
max_crowdsourced_pm <- max(crowdsourced$PM)

# Print 
print(paste("Crowdsourced PM max is", max_crowdsourced_pm, "µg/m³"))

# Find Pm sample max of govt 
max_govt_pm <- max(govt$PM)

# Print  
print(paste("Government PM max is", max_govt_pm, "µg/m³"))
```

2.  Discuss any key differences that you see between these two samples.

The govt data set shows about half of average pollution than the crowd sourced.

3.  Are the differences in mean pollution as expected, given what we know about the sampling strategies?

Yes this was expected. The government is cookin' the books. In other words, they are possibly manipulating the data by selecting more favorable locations. 

### Question 4

Use the location of the air pollution stations for both of the sampling strategies to generate a map showing locations of each observation. Color the two samples with different colors to highlight how each sample obtains measurements from different parts of the city.

```{r}
# Use ggplot to map air pollution locations
air_mon_location <- ggplot() +
  geom_point(data = crowdsourced, 
             aes(x = longitude, 
                 y = latitude), 
                 color = "red") +
  geom_point(data = govt, 
             aes(x = longitude, 
                 y = latitude), 
                 color = "blue") + 
  labs(title = "Air Monitor locations around Lahore, Pakistan", x = "Longitude", y = "Latitude")

print(air_mon_location)
```


::: callout-tip
`longitude` indicates location in the *x*-direction, while `latitude` indicates location in the *y*-direction. With `ggplot2` this should be nothing fancy. We'll do more spatial data in `R` later in the course.
:::

### Question 5

The local newspaper in Pakistan, *Dawn*, claims that the government is misreporting the air pollution levels in Lahore. Do the locations of monitors in question 4, relative to crowd-sourced monitors, suggest anything about a possible political bias?

Yes. The government seems to me monitoring in a specific location, which is around 31.58 latitude and 74.330 in longitude. This cluster is tighter when compared with the crowd sourced. 

### Question 6

Given the recent corruption in air quality reporting, the Prime Minister of Pakistan has hired an independent body of environmental data scientists to create an unbiased estimate of the mean PM 2.5 across Lahore using some combination of both government stations and crowd sourced observations.

NASA's satellite data indicates that the average PM across Lahore is 89.2 $\mu g/m^3$. Since this is the most objective estimate of population-level PM 2.5 available, your goal is to match this mean as closely as possible by creating a new ground-level monitoring sample that draws on both the government and crowd-sourced samples.

#### Question 6.1

First, generate a *random sample* of size $n=1000$ air pollution records by (i) pooling observations across the government and the crowd-sourced data; and (ii) drawing observations at random from this pooled sample.

```{r}

# Use bind_rows to pool data from both data frames 
pooled_data <- bind_rows(govt, crowdsourced)

# Draw a random sample of size 1000
random_sample <- pooled_data %>% sample_n(1000)

# Find mean of random_sample.  
mean_random_sample <- mean(random_sample$PM)

# Print
print(paste("The mean of the random sample is",mean_random_sample, "µg/m³"))

```


::: callout-tip
`bind_rows()` may be helpful.
:::

Second, create a *stratified random sample*. Do so by (i) stratifying your pooled data-set into strata of 0.01 degrees of latitude, and (ii) randomly sampling 200 air pollution observations from each stratum.

```{r}
# Create strata by rounding the latitude to 0.01 degrees
pooled_data <- pooled_data %>%
  mutate(lat_stratum = round(latitude, 2))

# Sample 200 observations from each stratum
stratified_sample <- pooled_data %>%
  group_by(lat_stratum) %>%
  sample_n(200) %>%
  ungroup()

# Find mean of stratified sample 
mean_strat_sample <- mean(stratified_sample$PM)

# Print
print(paste("The mean of the strat sample is", mean_strat_sample, "µg/m³"))

```

#### Question 6.2

Compare estimated means of PM 2.5 for each sampling strategy to the NASA estimate of 89.2 $\mu g/m^3$. Which sample seems to match the satellite data best? What would you recommend the Prime Minister do? Does your proposed sampling strategy rely more on government or on crowd-sourced data? Why might that be the case?

It would seem that the stratified sample matches the NASA estimate the best. This would make sense because it reduces some of the biases from the government monitors, which are all in close proximity to 31.58 latitude. I would recommend the Prime Minister to use stratified sampling because it will rely more on crowd sourced data since it is more spread out in latitude across Lahore, giving a more accurate representation of air pollution. 
